# -*- coding: utf-8 -*-
#"""CSC420_Project_MultiSlam.ipynb

#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/1lAn0YT0-ymp0Gay0LU-vTBPgd4Ny1bOU
#"""

# Mount Google drive
#from google.colab import drive
#drive.mount('/content/drive')

# Create link in colab's home folder
#!ln -s 'drive/My Drive/CSC420_Project_MultiSlam' CSC420_Project_MultiSlam

# !git clone https://github.com/facebookresearch/detectron2 detectron2_repo
# !git clone https://github.com/nianticlabs/monodepth2.git
# !git clone https://github.com/ucbdrive/hd3.git
# !pip install -U 'git+https://github.com/facebookresearch/fvcore.git'
# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
# !pip install -e detectron2_repo
# !pip install -U "git+https://github.com/cocodataset/panopticapi.git"

# !pip install -U "git+https://github.com/lvis-dataset/lvis-api.git"
# !pip install -U "pip install git+https://github.com/mcordts/cityscapesScripts.git"
# !pip install -U "git+https://github.com/mcordts/cityscapesScripts.git"

# !pip install -U \
# absl-py==0.8.1 \
# astroid==2.3.3 \
# attrs==19.3.0 \
# backcall==0.1.0 \
# bleach==3.1.0 \
# cachetools==3.1.1 \
# certifi==2019.11.28 \
# chardet==3.0.4 \
# cityscapesScripts==1.1.0 \
# Click==7.0 \
# cloudpickle==1.2.2 \
# cupy==6.5.0 \
# cycler==0.10.0 \
# Cython==0.29.12 \
# decorator==4.4.1 \
# defusedxml==0.6.0 \
# entrypoints==0.3 \
# fastrlock==0.4 \
# Flask==1.1.1 \
# fvcore==0.1 \
# google-auth==1.7.1 \
# google-auth-oauthlib==0.4.1 \
# grpcio==1.25.0 \
# idna==2.8 \
# imageio==2.6.1 \
# imagesize==1.1.0 \
# importlib-metadata==1.0.0 \
# imutils==0.5.3 \
# ipykernel==5.1.3 \
# ipython==7.10.0 \
# ipython-genutils==0.2.0 \
# ipywidgets==7.5.1 \
# isort==4.3.21 \
# itsdangerous==1.1.0 \
# jedi==0.15.1 \
# Jinja2==2.10.3 \
# joblib==0.14.0 \
# jsonschema==3.2.0 \
# jupyter-client==5.3.4 \
# jupyter-core==4.6.1 \
# kiwisolver==1.1.0 \
# lazy-object-proxy==1.4.3 \
# lvis==0.5.1 \
# Markdown==3.1.1 \
# MarkupSafe==1.1.1 \
# matplotlib==3.1.1 \
# mccabe==0.6.1 \
# mistune==0.8.4 \
# more-itertools==8.0.0 \
# nbconvert==5.6.1 \
# nbformat==4.4.0 \
# networkx==2.4 \
# notebook==6.0.2 \
# numpy==1.16.4 \
# oauthlib==3.1.0 \
# open3d==0.8.0.0 \
# opencv-contrib-python==3.4.2.17 \
# opencv-python==3.4.2.17 \
# pandocfilters==1.4.2 \
# panopticapi==0.1 \
# parso==0.5.1 \
# pexpect==4.7.0 \
# pickleshare==0.7.5 \
# Pillow==6.2.1 \
# portalocker==1.5.2 \
# prometheus-client==0.7.1 \
# prompt-toolkit==3.0.2 \
# protobuf==3.11.0 \
# ptyprocess==0.6.0 \
# pyasn1==0.4.8 \
# pyasn1-modules==0.2.7 \
# pycocotools==2.0 \
# Pygments==2.5.2 \
# pylint==2.4.4 \
# pyparsing==2.4.0 \
# pyrsistent==0.15.6 \
# python-dateutil==2.8.0 \
# PyWavelets==1.1.1 \
# PyYAML==5.1.2 \
# pyzmq==18.1.1 \
# requests==2.22.0 \
# requests-oauthlib==1.3.0 \
# rsa==4.0 \
# scikit-image==0.16.2 \
# scikit-learn==0.21.3 \
# scipy==1.3.2 \
# Send2Trash==1.5.0 \
# six==1.12.0 \
# tabulate==0.8.6 \
# tensorboard==2.0.2 \
# termcolor==1.1.0 \
# terminado==0.8.3 \
# testpath==0.4.4 \
# torch==1.3.1 \
# torchvision==0.4.2 \
# tornado==6.0.3 \
# tqdm==4.39.0 \
# traitlets==4.3.3 \
# typed-ast==1.4.0 \
# urllib3==1.25.7 \
# wcwidth==0.1.7 \
# webencodings==0.5.1 \
# Werkzeug==0.16.0 \
# widgetsnbextension==3.5.1 \
# wrapt==1.11.2 \
# yacs==0.1.6 \
# youtube-dl==2019.11.28 \
# zipp==0.6.0

# !cd monodepth2 && python3 test_simple.py --image_path assets/test_image.jpg --model_name mono_1024x320

from __future__ import absolute_import, division, print_function
import sys
sys.path.insert(1, 'hd3_repo')
sys.path.insert(1, 'monodepth2_repo')

import os
from os.path import join
import threading
import argparse
import datetime
import imutils
import time
import copy
import cv2
import logging
import pprint
from argparse import ArgumentParser
from tqdm import tqdm
import numpy as np
import open3d as o3d
import PIL.Image as pil
from PIL import Image
import matplotlib as mpl
import matplotlib.cm as cm
import matplotlib.pyplot as plt

import torchvision
import torch
import torch.nn.parallel
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
import torch.optim
import torch.utils.data
from torchvision import transforms, datasets

import detectron2
from detectron2.utils.logger import setup_logger
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

import monodepth2_repo.networks as networks
from monodepth2_repo.layers import disp_to_depth
from monodepth2_repo.utils import download_model_if_doesnt_exist

# import hd3.data.hd3data as datasets
# import hd3.data.flowtransforms as transforms
# import hd3.hd3model as models
# from hd3.utils.utils import *
# from hd3.models.hd3_ops import *
# import hd3.utils.flowlib as fl

class AgentLocate:
    def __init__(self, kpNum=1000):
        self.kpNum = kpNum
        self.sift = cv2.xfeatures2d.SIFT_create(self.kpNum)
        self.FLANN_INDEX_KDTREE = 1
        self.index_params = dict(algorithm = self.FLANN_INDEX_KDTREE, trees = 5)
        self.search_params = dict(checks=50)
        self.flann = cv2.FlannBasedMatcher(self.index_params,self.search_params)
        self.match_thresh = 5

    def getTrainDescriptors(self):
        return self.flann.getTrainDescriptors()

    def isApart(self, kp, kpList, thresh=50):
        cur_pt = np.array(kp.pt)

        for i in kpList:
            i_pt = np.array(i.pt)
            if np.linalg.norm(cur_pt - i_pt) < thresh:
                return False
        return True

    def estimate(self, img, kpNum=30):
        kp, des = self.sift.detectAndCompute(img,None)

        # sort keypoints by response
        kp, des = zip(*(sorted(zip(kp, des), key=lambda x: x[0].response, reverse=True)))
        
        # space apart keypoints
        kp_apart = []
        des_apart = []
        for i in range(len(kp)):
            if self.isApart(kp[i], kp_apart):
                kp_apart.append(kp[i])
                des_apart.append(des[i])

        if kpNum <= len(kp_apart):
            kp = kp_apart[:kpNum]
            des = des_apart[:kpNum]
        else:
            kp = kp_apart
            des = des_apart

        # matches = self.flann.radiusMatch(np.array(des), maxDistance=50)
        matches = self.flann.knnMatch(np.array(des), k=1)
        print([x[0].imgIdx for x in matches])
        matches = [x for x in matches if x[0].distance < 200]
        print(len(matches))
        # if len(matches) > 0:
        #     for i in range(len(matches)):
        #         print(matches[i][0].distance)

        self.flann.add([np.array(des)])
        self.flann.train()
        return (kp, matches)
        # return cv2.drawKeypoints(img,kp,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

class MonoDepth:
    def __init__(self):
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")
        
        self.model_path = os.path.join("monodepth2_repo/models", "mono_1024x320")
        # print("-> Loading model from ", self.model_path)
        self.encoder_path = os.path.join(self.model_path, "encoder.pth")
        self.depth_decoder_path = os.path.join(self.model_path, "depth.pth")
        
        # LOADING PRETRAINED MODEL
        # print("   Loading pretrained encoder")
        self.encoder = networks.ResnetEncoder(18, False)
        self.loaded_dict_enc = torch.load(self.encoder_path, map_location=self.device)

        # extract the height and width of image that this model was trained with
        self.feed_height = self.loaded_dict_enc['height']
        self.feed_width = self.loaded_dict_enc['width']
        self.filtered_dict_enc = {k: v for k, v in self.loaded_dict_enc.items() if k in self.encoder.state_dict()}
        self.encoder.load_state_dict(self.filtered_dict_enc)
        self.encoder.to(self.device)
        self.encoder.eval()

        # print("   Loading pretrained decoder")
        self.depth_decoder = networks.DepthDecoder(
            num_ch_enc=self.encoder.num_ch_enc, scales=range(4))

        self.loaded_dict = torch.load(self.depth_decoder_path, map_location=self.device)
        self.depth_decoder.load_state_dict(self.loaded_dict)

        self.depth_decoder.to(self.device)
        self.depth_decoder.eval()

    def estimate(self, img):
        with torch.no_grad():
            # for idx, image_path in enumerate(paths):

            #     if image_path.endswith("_disp.jpg"):
            #         # don't try to predict disparity for a disparity image!
            #         continue

            #     # Load image and preprocess
            #     img = pil.open(image_path).convert('RGB')
            img = Image.fromarray(img).convert('RGB')
            original_width, original_height = img.size
            img = img.resize((self.feed_width, self.feed_height), pil.LANCZOS)
            img = transforms.ToTensor()(img).unsqueeze(0)

            # PREDICTION
            img = img.to(self.device)
            features = self.encoder(img)
            outputs = self.depth_decoder(features)

            disp = outputs[("disp", 0)]
            disp_resized = torch.nn.functional.interpolate(
                disp, (original_height, original_width), mode="bilinear", align_corners=False)

            # # Saving numpy file
            # output_name = os.path.splitext(os.path.basename(image_path))[0]
            # name_dest_npy = os.path.join(output_directory, "{}_disp.npy".format(output_name))
            # scaled_disp, _ = disp_to_depth(disp, 0.1, 100)
            # np.save(name_dest_npy, scaled_disp.cpu().numpy())

            # Saving colormapped depth imaged
            disp_resized_np = disp_resized.squeeze().cpu().numpy()
            vmax = np.percentile(disp_resized_np, 95)
            normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)
            mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')
            colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)
            
            return colormapped_im

class StereoDepth:
    def __init__(self, model_path):
        self.corr_range = [4, 4, 4, 4, 4, 4]
        self.model = models.HD3Model("stereo", "dlaup", "hda", self.corr_range,
                                False).cuda()
        self.model = torch.nn.DataParallel(self.model).cuda()
        
        self.checkpoint = torch.load(model_path)
        self.model.load_state_dict(self.checkpoint['state_dict'], strict=True)
        
        self.model.eval()

    def get_target_size(self, H, W):
        h = 64 * np.array([[math.floor(H / 64), math.floor(H / 64) + 1]])
        w = 64 * np.array([[math.floor(W / 64), math.floor(W / 64) + 1]])
        ratio = np.abs(np.matmul(np.transpose(h), 1 / w) - H / W)
        index = np.argmin(ratio)
        return h[0, index // 2], w[0, index % 2]

    def estimate(self, imgL, imgR):
        input_size = imgL.shape

        # imgL = Image.fromarray(imgL).convert('RGB')
        # imgR = Image.fromarray(imgR).convert('RGB')
        # imgL = Image.fromarray(imgL)
        # imgR = Image.fromarray(imgR)
        # print(imgR.size)

        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        
        th, tw = self.get_target_size(input_size[0], input_size[1])
        # print(th, tw)
        
        val_transform = transforms.Compose(
            [transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std)])
        
        # val_data = datasets.HD3Data(
        mode="stereo"
        # data_root=args.data_root,
        # data_list=args.data_list,
        label_num=False
        transform=val_transform
        out_size=True
            # )
        
        # val_loader = torch.utils.data.DataLoader(
        #     val_data,
        #     batch_size=1,
        #     shuffle=False,
        #     num_workers=16,
        #     pin_memory=True)

        cudnn.enabled = True
        cudnn.benchmark = True
        
        # imgL = val_transform(imgL, [])
        # imgR = val_transform(imgR, [])
        # print(imgL.shape)
        cur_data = [[imgL, imgR], []]
        cur_data = list(val_transform(*cur_data))
        cur_data.append(np.array(input_size[::-1], dtype=int))
        cur_data = tuple(cur_data)

        # print(cur_data)
        # imgL_tensor = cur_data[0][0]
        # print(imgL)
        # imgR_tensor = cur_data[0][1]

        with torch.no_grad():
            # for i, (img_list, label_list, img_size) in enumerate(val_loader):
                # data_time.update(time.time() - end)

            img_list, label_list, img_size = cur_data
            # print(img_list)
            img_list = [img.unsqueeze(0) for img in img_list]
            img_size = np.array(input_size[:2][::-1], dtype=int)
            img_size = img_size[np.newaxis, :]
            # print(img_size)
            img_list = [img.to(torch.device("cuda")) for img in img_list]
            label_list = [
                label.to(torch.device("cuda")) for label in label_list
            ]

            # resize test
            resized_img_list = [
                F.interpolate(
                    img, (th, tw), mode='bilinear', align_corners=True)
                for img in img_list
            ]



            # # img_size = np.array(imgL.size, dtype=int).cpu().numpy()
            # img_size = np.array(imgL.size, dtype=int)
            # img_list = [imgL_tensor.to(torch.device("cuda")), imgR_tensor.to(torch.device("cuda"))]
            # label_list = []
            # print(img_list[0].shape)

            # # resize test
            # resized_img_list = [
            #     F.interpolate(
            #         img, (th, tw), mode='bilinear', align_corners=True)
            #     for img in img_list
            # ]


            output = self.model(
                img_list=resized_img_list,
                label_list=label_list,
                get_vect=True,
                get_epe=False)
            scale_factor = 1 / 2**(7 - len(self.corr_range))
            output['vect'] = resize_dense_vector(output['vect'] * scale_factor,
                                                img_size[0, 1],
                                                img_size[0, 0])


            pred_vect = output['vect'].data.cpu().numpy()
            pred_vect = np.transpose(pred_vect, (0, 2, 3, 1))
            # curr_bs = pred_vect.shape[0]
            # assert curr_bs == 1

            # for idx in range(curr_bs):
                # curr_idx = i * 1 + idx
            curr_vect = pred_vect[0]

            vis_flo = fl.flow_to_image(fl.disp2flow(curr_vect))
            vis_flo = cv2.cvtColor(vis_flo, cv2.COLOR_RGB2BGR)
            return vis_flo

                    # cv2.imwrite(vis_fn, vis_flo)

                    # cv2.imwrite(vect_fn,
                    #             np.uint16(-curr_vect[:, :, 0] * 256.0))

class GeoProjection():
    def __init__(self, mode='offline'):
        self.mode = mode
        self.pcd = None
        self.z = 0
        self.vis = None

        if self.mode == 'online':
            self.vis = o3d.visualization.Visualizer()
            self.vis.create_window()

    # def init_geometry(self, img):
    #     self.vis.add_geometry(pcd2)
    
    def update(self):
        self.vis.update_geometry()
        self.vis.poll_events()
        self.vis.update_renderer()

    def estimate(self, img_colour, img_depth, crop_fact_h=0.8, crop_fact_w=0.7, downsample=20):
        # crop
        h, w = img_colour.shape[:2]
        crop_h = int((h - (crop_fact_h*h)) / 2)
        crop_w = int((w - (crop_fact_w*w)) / 2)
        
        # print(h, w)

        img_colour = copy.deepcopy(img_colour[crop_h:h-crop_h, crop_w:w-crop_w, :])
        img_od3_colour = o3d.geometry.Image(img_colour)
        img_depth = copy.deepcopy(img_depth[crop_h:h-crop_h, crop_w:w-crop_w, :])
        img_od3_depth = o3d.geometry.Image(img_depth)

        # o3d.visualization.draw_geometries([img_od3_colour])
        # o3d.visualization.draw_geometries([img_od3_depth])
        
        rgbd_img = o3d.geometry.RGBDImage.create_from_color_and_depth(img_od3_colour, img_od3_depth)

        cur_pcd = o3d.geometry.PointCloud.create_from_rgbd_image(
            rgbd_img,
            o3d.camera.PinholeCameraIntrinsic(
                o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault))

        cur_pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])
        cur_pcd = cur_pcd.uniform_down_sample(downsample)
        cur_pcd.translate([0,0,self.z])
        self.z -= 0.00001
        
        if self.pcd == None:
            self.pcd = copy.deepcopy(cur_pcd)
            
            if self.mode == 'online':
                self.vis.add_geometry(self.pcd)
        else:
            # self.pcd.points = cur_pcd.points
            # self.pcd.colors = cur_pcd.colors
            # self.pcd.normals = cur_pcd.normals
            self.pcd += cur_pcd
            # self.pcd = copy.deepcopy(cur_pcd)
            # self.vis.add_geometry(self.pcd)
        
        if self.mode == 'online':
            self.update()
            return None
        else:
            return self.pcd

class ObjectDetect:
    def __init__(self, model_yaml):
        self.cfg = get_cfg()
        self.cfg.merge_from_file(model_yaml)
        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
        self.cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
        self.predictor = DefaultPredictor(self.cfg)
    
    def estimate(self, img):
        outputs = self.predictor(img)
        v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.0)
        v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
        return v.get_image()[:, :, ::-1]
    
    def getCFG(self):
        return self.cfg

debugging = False
length = 0

enableModules = {
    'object_detection': True,
    'stereo_depth': False,
    'agent_locate': False,
    'mono_depth': True,         # <----
    'geo_projection': False,     # -----^ dependency
}

outputFrame = {
    'object_detection': None,
    'mono_depth': None,
    'stereo_depth': None,
    'agent_locate': None,
    'geo_projection': None
}

outputWriter = {}

vs = None   # Left camera stream
vs2 = None  # Right camera stream

def mslam():
    global vs, enableModules, outputFrame, length
    
    # Initialize modules
    mod_object_detection = None
    mod_mono_depth = None
    mod_stereo_depth = None
    mod_agent_locate = None
    mod_geo_projection = None

    if debugging:
        print("Initializing modules")

    if enableModules['object_detection']:
        mod_object_detection = ObjectDetect("detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
    if enableModules['mono_depth']:
        mod_mono_depth = MonoDepth()
    if enableModules['stereo_depth']:
        mod_stereo_depth = StereoDepth("hd3_repo/scripts/model_zoo/hd3s_things_kitti-1243813e.pth")
    if enableModules['agent_locate']:
        mod_agent_locate = AgentLocate()
    if enableModules['geo_projection']:
        mod_geo_projection = GeoProjection(mode='offline')

    if debugging:
        print("Done initializing modules")
        print("Starting MultiSlam rendering")

    for i in tqdm(range(length)):
        # Get next frame
        isValid, frame = vs.read()
        if not isValid: # No more frames
            break
        
        # Get next frame for right camera
        if enableModules['stereo_depth']:
            isValid, frame2 = vs2.read()
            if not isValid:
                break

        if enableModules['mono_depth']:
            outputFrame['mono_depth'] = mod_mono_depth.estimate(frame.copy())
        if enableModules['agent_locate']:
            outputFrame['agent_locate'] = mod_agent_locate.estimate(frame.copy())
        if enableModules['object_detection']:
            outputFrame['object_detection'] = mod_object_detection.estimate(frame.copy())
        if enableModules['geo_projection']:
            outputFrame['geo_projection'] = mod_geo_projection.estimate(frame.copy(), outputFrame['mono_depth'].copy(), downsample=1000)
        # print(hash(np.sum(outputFrame['object_detection'])))
        writeFrame(i)

def writeFrame(frameNum):
    global outputFrame, outputWriter, enableModules, debugging

    for m in enableModules.keys():

        if not enableModules[m] or outputFrame[m] is None:
            continue

        if m == 'geo_projection':
            o3d.io.write_point_cloud(outputWriter[m], outputFrame['geo_projection'])
            continue

        if debugging:
            print("writing frame", frameNum, "for", m)
            print("frame stats: ", np.max(outputFrame[m]), np.min(outputFrame[m]), outputFrame[m].shape)

        outputWriter[m].write(outputFrame[m])

# NOTE: No stream for geo_projection as it is rendered in the Open3D visualizer

# if __name__ == '__main__':
# Arg parsing
# ap = argparse.ArgumentParser()
# ap.add_argument("-L", "--leftcam", type=str, required=True, help="left camera stream")
# ap.add_argument("-R", "--rightcam", type=str, required=False, help="right camera stream")
# ap.add_argument("-O", "--output", type=str, required=True, help="output path")
# ap.add_argument("-e", "--endframe", type=int, required=False, help="end frame number")
# args = vars(ap.parse_args())
args = {
    'leftcam': 'video/trimmed.mp4',
    'rightcam': None,
    'output': 'OUTPUT/',
    'endframe': 30*2
}

# Camera feed setup
vs = cv2.VideoCapture(args["leftcam"])
if args["rightcam"] is not None:
    vs2 = cv2.VideoCapture(args["rightcam"])

# Get video stats
length = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))
width  = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))

if args["endframe"] is not None:
    length = args["endframe"]

# Ouput format
fourcc = cv2.VideoWriter_fourcc(*'mp4v')

# Output writers
for m in enableModules.keys():
    if enableModules[m]:
        if m is 'geo_projection':
            outputWriter[m] = args["output"]+'OUT_' + m +'.pcd'
        else:
            outputWriter[m] = cv2.VideoWriter(args["output"]+'OUT_' + m +'.mp4',fourcc,30,(width,height))

# Single threaded multislam
t = threading.Thread(target=mslam)
t.daemon = True
t.start()

# Wait for thread to finish
t.join()

print("Done processing video")

# # don't have to run this: !cd detectron2_repo && python3 tools/train_net.py --config-file configs/Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x.yaml --num-gpus 1

# # don't have to run this: !cd detectron2_repo && cd datasets && ./prepare_for_tests.sh
